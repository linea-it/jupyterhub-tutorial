{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ce5b93-7970-42c7-8651-d7602a1d418a",
   "metadata": {},
   "source": [
    "<img align='left' src = '../../images/linea.png' width=150 style='padding: 20px'> \n",
    "\n",
    "# LSDB Tutorial - TopN-Max and TailN-Min queries\n",
    "\n",
    "How to perform global topN max and tailN min queries. <br>\n",
    "\n",
    "Contact: Luigi Silva ([luigi.silva@linea.org.br](mailto:luigi.silva@linea.org.br)) <br>\n",
    "Last check: Dec. 01, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43609e63-5093-4686-8858-754c31ef11ad",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfcc68-8c83-447b-be3a-d3cbafe5e49c",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "In the standard [LSDB](https://docs.lsdb.io/en/latest/) access model (e.g., through [data.lsdb.io](data.lsdb.io)), each [HATS](https://hats.readthedocs.io/en/latest/) pixel corresponds to a standalone parquet file, and LSDB relies on PyArrow for reading these files. PyArrow currently requires downloading and parsing the entire parquet file before applying column or row-level filters. As a consequence, even when the user requests only a subset of columns or a simple row filter, the full parquet payload is transferred to the client and processed locally. In some cases, due to internal parquet metadata handling, the same file may even be read multiple times.\n",
    "\n",
    "When accessing a catalog through an LSDB-server instance (for example, via the ```:43210``` endpoint), a different model is used. The LSDB-server acts as a proxy layer over the underlying parquet files. In this configuration:\n",
    "\n",
    "* Column projection (```columns=[...]```) and\n",
    "\n",
    "* Row filtering via PyArrow-compatible predicates (```filters=[(...)]```)\n",
    "\n",
    "are executed on the server side, before any data is transmitted to the client.\n",
    "\n",
    "The server loads the parquet file locally, applies the filters and column selection using PyArrow, and only the resulting reduced subset is serialized and sent over HTTP to the client. This significantly reduces both network transfer volume and client-side memory usage, especially for large pixels or queries that discard most of the rows.\n",
    "\n",
    "The LSDB-server approach is still experimental, as it requires ensuring that different archive providers support the same HTTP semantics and PyArrow filter behaviors. However, for workloads involving selective access to a few columns or strongly filtering predicates, server-side processing offers substantial performance improvements compared to the raw-file mode of data.lsdb.io."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b85714-b713-4796-8813-90a5ca7f505b",
   "metadata": {},
   "source": [
    "### Main parameters in the codes below\n",
    "\n",
    "The codes perform global Top-N (maximum values) or Tail-N (minimum values) searches based on a selected ranking column. The parameters below define how many partitions are inspected and how strict the server-side filtering is.\n",
    "\n",
    "**K**  \n",
    "Number of HATS partitions to inspect, after sorting all partitions by the relevant statistic (`max_value` for Top-N, `min_value` for Tail-N).  \n",
    "If **K is too small**, the algorithm may fail to assemble the requested N results.  \n",
    "If **K is too large**, too many partitions will be loaded, increasing memory usage and slowing down the run.\n",
    "\n",
    "**topN_max_count / tailN_min_count**  \n",
    "Number of rows expected in the final global Top-N or Tail-N result.  \n",
    "The code validates that the final output contains exactly N objects.\n",
    "\n",
    "**ranking_column**  \n",
    "Column used to extract global maxima or minima. Must be included in the projected columns.\n",
    "\n",
    "**ranking_threshold / ranking_threshold_min**  \n",
    "Server-side row filter.  \n",
    "Top-N keeps `ranking_column >= threshold`  \n",
    "Tail-N keeps `ranking_column <= threshold_min`  \n",
    "If **too restrictive**, required candidates may be discarded.  \n",
    "If **too permissive**, the filtered partitions may become very large, increasing the risk of excessive memory usage or local compute overload.\n",
    "\n",
    "**General guidance**  \n",
    "Choose **K** reasonably close to **N**, but not equal to N.  \n",
    "Select a **threshold** that is loose enough to retain the needed candidates, but tight enough to keep data volumes manageable.  \n",
    "Both parameters affect memory footprint and correctness, and the final validation steps will fail if the selection is incomplete or unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e07aa-1e67-4c44-8041-88530a5b8f08",
   "metadata": {},
   "source": [
    "## Imports and Cluster Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b721ef5-b64f-430b-bdbc-400e4ac3b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lsdb\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Start local Dask cluster\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Starting local Dask cluster...\")\n",
    "cluster = LocalCluster(\n",
    "    n_workers=3,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit=\"4GB\",\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(\">> Cluster started.\")\n",
    "print(\"   Dashboard:\", client.dashboard_link)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b6808-577d-4fc6-8d7d-9e22a4987faa",
   "metadata": {},
   "source": [
    "## TopN-Max query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3354d0-8885-4787-8e9d-1085e7b8252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL TIMER\n",
    "t_global_start = time.perf_counter()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 0) Configurable parameters\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Number of HATS partitions to inspect when searching for the\n",
    "# global highest values of the ranking column\n",
    "K = 90\n",
    "\n",
    "# Number of highest-valued rows to return (global top-N maximum search)\n",
    "topN_max_count = 100\n",
    "\n",
    "# Column whose global maximum values will be extracted\n",
    "ranking_column = \"parallax\"\n",
    "\n",
    "# Server-side cutoff; rows below this value are discarded before transfer\n",
    "ranking_threshold = 155\n",
    "\n",
    "# Columns to request from LSDB (server-side projection)\n",
    "columns_to_select = [\"ra\", \"dec\", \"phot_g_mean_mag\", \"parallax\"]\n",
    "\n",
    "print(f\">> K = {K}\")\n",
    "print(f\">> topN_max_count = {topN_max_count}\")\n",
    "print(f\">> ranking_column = '{ranking_column}'\")\n",
    "print(f\">> ranking_threshold = {ranking_threshold}\")\n",
    "print(f\">> columns_to_select = {columns_to_select}\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# Ensure ranking_column is included\n",
    "if ranking_column not in columns_to_select:\n",
    "    print(f\">> Adding ranking_column '{ranking_column}' to columns_to_select.\")\n",
    "    columns_to_select.append(ranking_column)\n",
    "\n",
    "# Fail fast: K cannot exceed topN_max_count in a global-top-N maximum search\n",
    "if K > topN_max_count:\n",
    "    raise ValueError(\n",
    "        f\"\\nERROR: Invalid parameter choice: K ({K}) > topN_max_count ({topN_max_count}).\\n\"\n",
    "        f\"K defines how many highest-max-value partitions will be inspected, and cannot\\n\"\n",
    "        f\"be larger than the number of global top-N results requested.\\n\"\n",
    "        f\"Please reduce K or increase topN_max_count.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) Open catalog with server-side projection\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Opening Gaia DR3 catalog (server-side projection enabled)...\")\n",
    "cat = lsdb.open_catalog(\n",
    "    \"http://epyc.astro.washington.edu:43210/hats/gaia_dr3\",\n",
    "    columns=columns_to_select,\n",
    ")\n",
    "print(\">> Catalog opened.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Per-pixel statistics (metadata only)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Reading per-pixel statistics...\")\n",
    "per_pixel = cat.per_pixel_statistics(\n",
    "    include_columns=[ranking_column],\n",
    "    include_stats=[\"max_value\"],\n",
    ")\n",
    "print(\">> Per-pixel statistics loaded.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# Sort partitions by descending max(ranking_column)\n",
    "per_pixel_sorted = per_pixel.sort_values(f\"{ranking_column}: max_value\", ascending=False)\n",
    "\n",
    "# Select top-K partitions that contain the highest maximum values of the column\n",
    "topK_pixels = per_pixel_sorted.head(K)\n",
    "pixel_list = topK_pixels.index.tolist()\n",
    "\n",
    "print(\">> Top K pixels (first 10):\")\n",
    "print(topK_pixels.head(10))\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) Server-side row filtering (first stage of global max search)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(f\">> Re-opening catalog with server-side filter ({ranking_column} >= {ranking_threshold})...\")\n",
    "cat_filtered = lsdb.open_catalog(\n",
    "    \"http://epyc.astro.washington.edu:43210/hats/gaia_dr3\",\n",
    "    columns=columns_to_select,\n",
    "    filters=[(ranking_column, \">=\", float(ranking_threshold))],\n",
    ")\n",
    "print(\">> Filtered catalog ready.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Restrict to the selected K partitions (second stage of max-value pruning)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(f\">> Applying pixel_search to the selected {K} partitions...\")\n",
    "cat_filtered_limited = cat_filtered.pixel_search(pixels=pixel_list)\n",
    "print(\">> Pixel subset applied.\")\n",
    "print(\"   Pixels:\", pixel_list[:10], \"...\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) Materialize candidate rows (after both filters)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Computing candidate dataset...\")\n",
    "df_candidates = cat_filtered_limited.compute()\n",
    "print(\">> Compute complete.\")\n",
    "print(\"   Rows returned:\", len(df_candidates))\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "elapsed = t1 - t0\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = elapsed % 60\n",
    "\n",
    "print(f\"   [Time: {elapsed:.2f} s | {minutes} min {seconds:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) Compute the global Top-N maximum values\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(f\">> Selecting global top-{topN_max_count} highest values of '{ranking_column}'...\")\n",
    "topN_global_max = df_candidates.nlargest(topN_max_count, ranking_column)\n",
    "print(topN_global_max.head(10))\n",
    "\n",
    "pN_min = topN_global_max[ranking_column].min()\n",
    "print(f\"\\n>> Minimum value within global top-{topN_max_count} = {pN_min}\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7) Validation checks (completeness of the top-N maxima)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Validating top-N maximum extraction...\")\n",
    "\n",
    "# Check final size\n",
    "if len(topN_global_max) != topN_max_count:\n",
    "    raise ValueError(\n",
    "        f\"\\nERROR: Expected {topN_max_count} objects but obtained {len(topN_global_max)}. \"\n",
    "        f\"Threshold or K may be too restrictive.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"   ✔ Size check passed ({topN_max_count} objects).\")\n",
    "\n",
    "# Ensure no omitted partition could surpass the smallest selected maximum\n",
    "if K < len(per_pixel_sorted):\n",
    "    next_partition_max = per_pixel_sorted[f\"{ranking_column}: max_value\"].iloc[K]\n",
    "else:\n",
    "    next_partition_max = -float(\"inf\")\n",
    "\n",
    "print(f\"   max_value of pixel K+1 = {next_partition_max}\")\n",
    "\n",
    "if pN_min < next_partition_max:\n",
    "    raise ValueError(\n",
    "        f\"\\nERROR: Potential missing objects: min(top-{topN_max_count}) = {pN_min} < \"\n",
    "        f\"max_value of pixel K+1 = {next_partition_max}. Increase K or adjust threshold.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"   ✔ Pixel-range validation passed.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GLOBAL EXECUTION TIME\n",
    "# -----------------------------------------------------------\n",
    "t_global_end = time.perf_counter()\n",
    "elapsed_global = t_global_end - t_global_start\n",
    "minutes_g = int(elapsed_global // 60)\n",
    "seconds_g = elapsed_global % 60\n",
    "\n",
    "print(f\"\\n>> TOTAL EXECUTION TIME: {elapsed_global:.2f} s | {minutes_g} min {seconds_g:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1b5a8-e6bf-4b7a-8863-95affd85865b",
   "metadata": {},
   "source": [
    "## TailN-Min query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f61c8-58e5-4630-af89-040fc6db7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL TIMER\n",
    "t_global_start = time.perf_counter()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 0) Configurable parameters\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Number of HATS partitions to inspect when searching for the\n",
    "# global lowest values of the ranking column\n",
    "K = 90\n",
    "\n",
    "# Number of lowest-valued rows to return (global tail-N minimum search)\n",
    "tailN_min_count = 100\n",
    "\n",
    "# Column whose global minimum values will be extracted\n",
    "ranking_column = \"parallax\"\n",
    "\n",
    "# Server-side cutoff; rows ABOVE this value are discarded before transfer\n",
    "# (i.e., we keep ranking_column <= ranking_threshold_min)\n",
    "ranking_threshold_min = -45\n",
    "\n",
    "# Columns to request from LSDB (server-side projection)\n",
    "columns_to_select = [\"ra\", \"dec\", \"phot_g_mean_mag\", \"parallax\"]\n",
    "\n",
    "print(f\">> K = {K}\")\n",
    "print(f\">> tailN_min_count = {tailN_min_count}\")\n",
    "print(f\">> ranking_column = '{ranking_column}'\")\n",
    "print(f\">> ranking_threshold_min = {ranking_threshold_min}\")\n",
    "print(f\">> columns_to_select = {columns_to_select}\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# Ensure ranking_column is included\n",
    "if ranking_column not in columns_to_select:\n",
    "    print(f\">> Adding ranking_column '{ranking_column}' to columns_to_select.\")\n",
    "    columns_to_select.append(ranking_column)\n",
    "\n",
    "# Fail fast: K cannot exceed tailN_min_count in a global tail-N minimum search\n",
    "if K > tailN_min_count:\n",
    "    raise ValueError(\n",
    "        f\"\\nERROR: Invalid parameter choice: K ({K}) > tailN_min_count ({tailN_min_count}).\\n\"\n",
    "        f\"K defines how many lowest-min-value partitions will be inspected, and cannot\\n\"\n",
    "        f\"be larger than the number of global tail-N results requested.\\n\"\n",
    "        f\"Please reduce K or increase tailN_min_count.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1) Open catalog with server-side projection\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Opening Gaia DR3 catalog (server-side projection enabled)...\")\n",
    "cat = lsdb.open_catalog(\n",
    "    \"http://epyc.astro.washington.edu:43210/hats/gaia_dr3\",\n",
    "    columns=columns_to_select,\n",
    ")\n",
    "print(\">> Catalog opened.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2) Per-pixel statistics (metadata only)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Reading per-pixel statistics...\")\n",
    "per_pixel = cat.per_pixel_statistics(\n",
    "    include_columns=[ranking_column],\n",
    "    include_stats=[\"min_value\"],\n",
    ")\n",
    "print(\">> Per-pixel statistics loaded.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# Sort partitions by ascending min(ranking_column)\n",
    "per_pixel_sorted = per_pixel.sort_values(f\"{ranking_column}: min_value\", ascending=True)\n",
    "\n",
    "# Select top-K partitions that contain the lowest minimum values of the column\n",
    "topK_pixels = per_pixel_sorted.head(K)\n",
    "pixel_list = topK_pixels.index.tolist()\n",
    "\n",
    "print(\">> Top K pixels (first 10):\")\n",
    "print(topK_pixels.head(10))\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3) Server-side row filtering (first stage of global min search)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(f\">> Re-opening catalog with server-side filter ({ranking_column} <= {ranking_threshold_min})...\")\n",
    "cat_filtered = lsdb.open_catalog(\n",
    "    \"http://epyc.astro.washington.edu:43210/hats/gaia_dr3\",\n",
    "    columns=columns_to_select,\n",
    "    filters=[(ranking_column, \"<=\", float(ranking_threshold_min))],\n",
    ")\n",
    "print(\">> Filtered catalog ready.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Restrict to the selected K partitions (second stage of min-value pruning)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(f\">> Applying pixel_search to the selected {K} partitions...\")\n",
    "cat_filtered_limited = cat_filtered.pixel_search(pixels=pixel_list)\n",
    "print(\">> Pixel subset applied.\")\n",
    "print(\"   Pixels:\", pixel_list[:10], \"...\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5) Materialize candidate rows (after both filters)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Computing candidate dataset...\")\n",
    "df_candidates = cat_filtered_limited.compute()\n",
    "print(\">> Compute complete.\")\n",
    "print(\"   Rows returned:\", len(df_candidates))\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "elapsed = t1 - t0\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = elapsed % 60\n",
    "\n",
    "print(f\"   [Time: {elapsed:.2f} s | {minutes} min {seconds:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6) Compute the global Tail-N minimum values\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(f\">> Selecting global tail-{tailN_min_count} lowest values of '{ranking_column}'...\")\n",
    "tailN_global_min = df_candidates.nsmallest(tailN_min_count, ranking_column)\n",
    "print(tailN_global_min.head(10))\n",
    "\n",
    "pN_max_in_tail = tailN_global_min[ranking_column].max()\n",
    "print(f\"\\n>> Maximum value within global tail-{tailN_min_count} = {pN_max_in_tail}\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7) Validation checks (completeness of the tail-N minima)\n",
    "# -----------------------------------------------------------\n",
    "t0 = time.perf_counter()\n",
    "print(\">> Validating tail-N minimum extraction...\")\n",
    "\n",
    "# Validation 1: Ensure we returned the requested count\n",
    "if len(tailN_global_min) != tailN_min_count:\n",
    "    raise ValueError(\n",
    "        f\"\\nERROR: Expected {tailN_min_count} objects but obtained {len(tailN_global_min)}. \"\n",
    "        f\"Threshold or K may be too restrictive.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"   ✔ Size check passed ({tailN_min_count} objects).\")\n",
    "\n",
    "# Validation 2: ensure no better (smaller) values exist in omitted partitions\n",
    "if K < len(per_pixel_sorted):\n",
    "    next_partition_min = per_pixel_sorted[f\"{ranking_column}: min_value\"].iloc[K]\n",
    "else:\n",
    "    next_partition_min = float(\"inf\")\n",
    "\n",
    "print(f\"   min_value of pixel K+1 = {next_partition_min}\")\n",
    "\n",
    "if pN_max_in_tail > next_partition_min:\n",
    "    raise ValueError(\n",
    "        f\"\\nERROR: Potential missing objects: max(tail-{tailN_min_count}) = {pN_max_in_tail} > \"\n",
    "        f\"min_value of pixel K+1 = {next_partition_min}. Increase K or adjust threshold.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"   ✔ Pixel-range validation passed.\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"   [Time: {t1 - t0:.2f} s]\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GLOBAL EXECUTION TIME\n",
    "# -----------------------------------------------------------\n",
    "t_global_end = time.perf_counter()\n",
    "\n",
    "elapsed_global = t_global_end - t_global_start\n",
    "minutes_g = int(elapsed_global // 60)\n",
    "seconds_g = elapsed_global % 60\n",
    "\n",
    "print(f\"\\n>> TOTAL EXECUTION TIME: {elapsed_global:.2f} s | {minutes_g} min {seconds_g:.2f} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Astro (mini)",
   "language": "python",
   "name": "astro-mini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
